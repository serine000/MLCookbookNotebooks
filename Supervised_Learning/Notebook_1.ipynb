{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.20.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Numpy Arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  -1.5  2.  -2.5]\n",
      " [ 0.   4.  -0.3  2.1]\n",
      " [ 1.   3.3 -1.9 -4.3]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [3, -1.5, 2, -2.5],\n",
    "    [0, 4, -0.3, 2.1],\n",
    "    [1, 3.3, -1.9, -4.3]\n",
    "])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "NpArray1 = np.arange(10) # Similar to the python Range function\n",
    "print(NpArray1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95]\n"
     ]
    }
   ],
   "source": [
    "NpArray2 = np.arange(10, 100, 5) # Array with elements from 10 -> 100 with a step of 5\n",
    "print(NpArray2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.20408163  0.40816327  0.6122449   0.81632653  1.02040816\n",
      "  1.2244898   1.42857143  1.63265306  1.83673469  2.04081633  2.24489796\n",
      "  2.44897959  2.65306122  2.85714286  3.06122449  3.26530612  3.46938776\n",
      "  3.67346939  3.87755102  4.08163265  4.28571429  4.48979592  4.69387755\n",
      "  4.89795918  5.10204082  5.30612245  5.51020408  5.71428571  5.91836735\n",
      "  6.12244898  6.32653061  6.53061224  6.73469388  6.93877551  7.14285714\n",
      "  7.34693878  7.55102041  7.75510204  7.95918367  8.16326531  8.36734694\n",
      "  8.57142857  8.7755102   8.97959184  9.18367347  9.3877551   9.59183673\n",
      "  9.79591837 10.        ]\n"
     ]
    }
   ],
   "source": [
    "NpArray3 = np.linspace(0, 10, 50) # Array of 50 equally spaced numbers between 0 & 10\n",
    "print(NpArray3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data preprocessing using mean removal**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean-removal or _Standardization_ is a technique that allows us to _scale_ the data based on **normal standard distribution**. It allows us to place our features on the same scale by making sure the data distribution now has a `mean = 0` and a `standard-deviation = 1`.\n",
    "\n",
    "This type of transformation changes the enitre **shape** of the data distribution. It's also referred to as _data standardization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 1.33333333  1.93333333 -0.06666667 -1.56666667]\n",
      "Standard deviation:  [1.24721913 2.44449495 1.60069429 2.69485106]\n"
     ]
    }
   ],
   "source": [
    "# Checking on the characteristics of our `data` array we created earlier.\n",
    "# axis = 0 means along each column, so 1.3333 corresponds to the mean of [3., 0. ,1.] from the first colum od `data`.\n",
    "\n",
    "print(\"Mean: \", data.mean(axis=0))\n",
    "print(\"Standard deviation: \", data.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardize = preprocessing.scale(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the mean values below are all ~ 0 and the STD is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -1.48029737e-16]\n",
      "Standard deviation:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \", data_standardize.mean(axis=0))\n",
    "print(\"Standard deviation: \", data_standardize.std(axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Scaling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to place our features on the same scale, we use a method called _data scaling_. \n",
    "\n",
    "Formally speaking: **_Scaling is the process of transforming data to fit on a specific scale._** This type of Transformation changes just the **range** of the data distribution.\n",
    "\n",
    "A well-known data scaling technique is called **min-max scaling**.\n",
    "\n",
    "A special case of Min-Max scaling where the target range for our data is between (0, 1) is also known as '_Normalization_' [! Not to confuse with vector normalization which we'll see in a second]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we are trying to fit our points within the (0,1) range.\n",
    "data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = data_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  [ 0.  -1.5 -1.9 -4.3]\n",
      "Max:  [3.  4.  2.  2.1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \", data.min(axis=0))\n",
    "print(\"Max: \", data.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  [0. 0. 0. 0.]\n",
      "Max:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \", data_scaled.min(axis=0))\n",
    "print(\"Max: \", data_scaled.max(axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare our original data values, to the scaled ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  -1.5  2.  -2.5]\n",
      " [ 0.   4.  -0.3  2.1]\n",
      " [ 1.   3.3 -1.9 -4.3]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         1.         0.28125   ]\n",
      " [0.         1.         0.41025641 1.        ]\n",
      " [0.33333333 0.87272727 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(data_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(Vector) Normalization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Vector) Normalization is a technique that allows us to transform the values in a vector in order for the vector to have a magnitude of 1 (basically trasnforming it into a _unit_ vector). This does not change the direction of the vector, just its magnitude.\n",
    "\n",
    "This method is also known as _Unit vector Scaling_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.75       -0.17045455  0.47619048 -0.28089888]\n",
      " [ 0.          0.45454545 -0.07142857  0.23595506]\n",
      " [ 0.25        0.375      -0.45238095 -0.48314607]]\n"
     ]
    }
   ],
   "source": [
    "# Normalization can be of different types, one of them being L1 which we are using in the method below.\n",
    "data_normalized = preprocessing.normalize(data, norm='l1', axis=0)\n",
    "print(data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "data_norms_abs = np.abs(data_normalized) # Here we took the abs value of each value along each column (the columns being our feature vectors).\n",
    "print(data_norms_abs.sum(axis=0)) # Here we summed the abs values we got from the previous step to show thta now each feature vector has a magnitude of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
