{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.20.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn) (1.8.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Numpy Arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  -1.5  2.  -2.5]\n",
      " [ 0.   4.  -0.3  2.1]\n",
      " [ 1.   3.3 -1.9 -4.3]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [3, -1.5, 2, -2.5],\n",
    "    [0, 4, -0.3, 2.1],\n",
    "    [1, 3.3, -1.9, -4.3]\n",
    "])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "NpArray1 = np.arange(10) # Similar to the python Range function\n",
    "print(NpArray1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95]\n"
     ]
    }
   ],
   "source": [
    "NpArray2 = np.arange(10, 100, 5) # Array with elements from 10 -> 100 with a step of 5\n",
    "print(NpArray2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.20408163  0.40816327  0.6122449   0.81632653  1.02040816\n",
      "  1.2244898   1.42857143  1.63265306  1.83673469  2.04081633  2.24489796\n",
      "  2.44897959  2.65306122  2.85714286  3.06122449  3.26530612  3.46938776\n",
      "  3.67346939  3.87755102  4.08163265  4.28571429  4.48979592  4.69387755\n",
      "  4.89795918  5.10204082  5.30612245  5.51020408  5.71428571  5.91836735\n",
      "  6.12244898  6.32653061  6.53061224  6.73469388  6.93877551  7.14285714\n",
      "  7.34693878  7.55102041  7.75510204  7.95918367  8.16326531  8.36734694\n",
      "  8.57142857  8.7755102   8.97959184  9.18367347  9.3877551   9.59183673\n",
      "  9.79591837 10.        ]\n"
     ]
    }
   ],
   "source": [
    "NpArray3 = np.linspace(0, 10, 50) # Array of 50 equally spaced numbers between 0 & 10\n",
    "print(NpArray3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data preprocessing using mean removal**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean-removal or _Standardization_ is a technique that allows us to _scale_ the data based on **normal standard distribution**. It allows us to place our features on the same scale by making sure the data distribution now has a `mean = 0` and a `standard-deviation = 1`.\n",
    "\n",
    "This type of transformation changes the enitre **shape** of the data distribution. It's also referred to as _data standardization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 1.33333333  1.93333333 -0.06666667 -1.56666667]\n",
      "Standard deviation:  [1.24721913 2.44449495 1.60069429 2.69485106]\n"
     ]
    }
   ],
   "source": [
    "# Checking on the characteristics of our `data` array we created earlier.\n",
    "# axis = 0 means along each column, so 1.3333 corresponds to the mean of [3., 0. ,1.] from the first colum od `data`.\n",
    "\n",
    "print(\"Mean: \", data.mean(axis=0))\n",
    "print(\"Standard deviation: \", data.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardize = preprocessing.scale(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the mean values below are all ~ 0 and the STD is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -1.48029737e-16]\n",
      "Standard deviation:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \", data_standardize.mean(axis=0))\n",
    "print(\"Standard deviation: \", data_standardize.std(axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Scaling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common technique to place our features on the same scale is called _data scaling_. \n",
    "\n",
    "Formally speaking: **_Scaling is the process of transforming data to fit on a specific scale._** This type of Transformation changes _just_ the **range** of the data distribution.\n",
    "\n",
    "A well-known data scaling technique is called **min-max scaling**.\n",
    "\n",
    "A special case of Min-Max scaling where the target range for our data is between (0, 1) is also known as '_Normalization_' scaling. \n",
    "\n",
    "[ ! Not to confuse with vector normalization which we'll see in a second which is a completely different type of transformation]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we are trying to fit our points within the (0,1) range.\n",
    "data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = data_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  [ 0.  -1.5 -1.9 -4.3]\n",
      "Max:  [3.  4.  2.  2.1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \", data.min(axis=0))\n",
    "print(\"Max: \", data.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  [0. 0. 0. 0.]\n",
      "Max:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Min: \", data_scaled.min(axis=0))\n",
    "print(\"Max: \", data_scaled.max(axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare our original data values, to the scaled ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  -1.5  2.  -2.5]\n",
      " [ 0.   4.  -0.3  2.1]\n",
      " [ 1.   3.3 -1.9 -4.3]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         1.         0.28125   ]\n",
      " [0.         1.         0.41025641 1.        ]\n",
      " [0.33333333 0.87272727 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(data_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(Vector) Normalization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Vector) Normalization is a technique that allows us to transform the values in a vector in order for the vector to have a magnitude of 1 (basically trasnforming it into a _unit_ vector). This does not change the direction of the vector, just its magnitude.\n",
    "\n",
    "This method is also known as _Unit vector Scaling_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.75       -0.17045455  0.47619048 -0.28089888]\n",
      " [ 0.          0.45454545 -0.07142857  0.23595506]\n",
      " [ 0.25        0.375      -0.45238095 -0.48314607]]\n"
     ]
    }
   ],
   "source": [
    "# Normalization can take on different types, one of them being L1 which we are using in the method below.\n",
    "data_normalized = preprocessing.normalize(data, norm='l1', axis=0)\n",
    "print(data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "data_norms_abs = np.abs(data_normalized) # Here we took the abs value of each value along each column (the columns being our feature vectors).\n",
    "print(data_norms_abs.sum(axis=0)) # Here we summed the abs values we got from the previous step to show that now: each feature (column) vector has a magnitude of 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binarization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarization is used to convert a feature vector to a boolean vector (0s & 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  -1.5  2.  -2.5]\n",
      " [ 0.   4.  -0.3  2.1]\n",
      " [ 1.   3.3 -1.9 -4.3]]\n"
     ]
    }
   ],
   "source": [
    "# Bringing back our data variable from previously\n",
    "data = np.array([\n",
    "    [3, -1.5, 2, -2.5],\n",
    "    [0, 4, -0.3, 2.1],\n",
    "    [1, 3.3, -1.9, -4.3]\n",
    "])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Values > threshold are mapped to 1, values <= threshold are mapped to 0\n",
    "# By default the threshold is set to 0, so only positive values are mapped to 1.\n",
    "data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)\n",
    "print(data_binarized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One-hot encoding**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an encoding technique that uses a _one-of-k_ scheme to encode values.\n",
    "\n",
    "So for example, if we have a unique set of colors [red, blue, yellow], then the color blue will be encoded as a vector [0, 1, 0], while the color red will be encoded as [1, 0, 0] and so on.<br />\n",
    "This technique is often employed to convert categorical data into numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2]\n",
      " [0 2 3]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1, 1, 2], [0, 2, 3], [1, 0, 1], [0, 1, 0]])\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first column we have two unique values: 0 & 1\n",
    "\n",
    "In the second feature we have 3 possibilities: 0, 1 & 2\n",
    "\n",
    "In the third feature column we have 4 possibilities: 0, 1, 2 & 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "encoder = preprocessing.OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "print(encoded_data.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value is now represented by the encoded version of its original value based on the unqiue values in each column.\n",
    "\n",
    "\n",
    "This is how we can also look at the encoded_data:<br />\n",
    "[[ 0. 1. 0. -- 1. 0. 0. -- 0. 1. 0. ]<br />\n",
    " [ 1. 0. 0. -- 0. 1. 0. -- 0. 0. 1. ]<br />\n",
    " [ 0. 1. 1. -- 0. 0. 0. -- 1. 0. 0. ]<br />\n",
    " [ 1. 0. 0. -- 1. 0. 1. -- 0. 0. 0. ]]<br />\n",
    "\n",
    "The value in position[0][0] which was '1' now maps to [0,1,0].<br />\n",
    "However the '1' in position [0][1] maps to a [1,0,0] because 1 is the first unqiue version in that column (column 1).<br />\n",
    "So the values get mapped based on the unique values in the column they are present in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Label encoding**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding is another encoding technique to transform word labels into numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "input_classes = ['audi', 'ford', 'audi', 'toyota', 'ford', 'bmw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: \n",
      "audi --> 0\n",
      "bmw --> 1\n",
      "ford --> 2\n",
      "toyota --> 3\n"
     ]
    }
   ],
   "source": [
    "label_encoder.fit(input_classes)\n",
    "print(\"Class mapping: \")\n",
    "for i, item in enumerate(label_encoder.classes_):\n",
    "    print(item, \"-->\", i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, the words have been transformed to zero-indexed numbers.<br />\n",
    "Now any word can be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels = ['toyota', 'ford', 'audi']\n",
      "Encoded labels = [3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "labels = ['toyota', 'ford', 'audi']\n",
    "encoded_labels = label_encoder.transform(labels)\n",
    "print(\"Labels =\", labels)\n",
    "print(\"Encoded labels =\", list(encoded_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even do an inverse transformation to map them back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels = [2, 1, 0, 3, 1]\n",
      "Decoded labels = ['ford', 'bmw', 'audi', 'toyota', 'bmw']\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = [2, 1, 0, 3, 1]\n",
    "decoded_labels = label_encoder.inverse_transform(encoded_labels)\n",
    "print(\"Encoded labels =\", encoded_labels)\n",
    "print(\"Decoded labels =\", list(decoded_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pros & Cons comparaison of Label encoding & One-hot encoding**\n",
    "While both methods are used to transform categorical data into numercial data, each method does things differently.\n",
    "- Label encoding gives labels based on ordinality, so it will map words in a way that two nearby values will be given similar encoding (Like audi & bmw in our example above, the letters a & b are next to each other in the alphabet).\n",
    "\n",
    "- One-hot encoding has the advantage that the result is binary rather than ordinal. In some cases this could lead to very large vector representation (like if we end up with 50+ unique values in a feature column).\n",
    "\n",
    "So it's really up to the problem that you're working on & which solution fits best your case for transforming categorical data to numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
